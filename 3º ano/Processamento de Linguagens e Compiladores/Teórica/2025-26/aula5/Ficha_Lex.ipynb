{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GdHi2G0OlbE"
      },
      "source": [
        "# Ficha de Análise Léxica (Lex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0JdE83EOlb0"
      },
      "source": [
        "## Introdução"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC00B5-IOlb1"
      },
      "source": [
        "A análise léxica é o processo de conversão de uma sequência de caracteres numa sequência de *tokens*, em que cada *token* representa uma unidade significativa da linguagem à qual os caracteres pertencem.\n",
        "\n",
        "Por exemplo:\n",
        "\n",
        "``` \n",
        "F = 32 + 1.8 * C\n",
        "```\n",
        "\n",
        "Daria a sequência de tokens:\n",
        "\n",
        "``` \n",
        "'F', '=', '32', '+', '1.8', '*', 'C' \n",
        "```\n",
        "\n",
        "Normalmente associamos aos tokens um nome com significado:\n",
        "\n",
        "```\n",
        "'ID', 'ATRIB', 'INT', 'SOMA', 'REAL', 'MUL', 'ID' \n",
        "```\n",
        "\n",
        "Mantendo também o valor do token que irá ser necessário para aqueles que são variáveis:\n",
        "\n",
        "```\n",
        "('ID', 'F'), ('ATRIB', '='), ('INT', '32'), ('SOMA', '+'), ('REAL', '1.8'), ('MUL', '*'), ('ID', 'C' \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Em Python, podemos fazer análise léxica de várias formas. A que iremos utilizar nas aulas recorre ao módulo Ply, que para além de análise léxica vai-nos permitir fazer análise sintática.\n",
        "\n",
        "Antes de usar o módulo Ply, precisamos de o instalar. Para isso, podemos usar o comando seguinte:\n",
        "\n",
        "```sh\n",
        "$ pip install ply\n",
        "```\n",
        "\n",
        "Depois, apenas precisamos de importar a ferramenta `lex.py` no nosso programa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yXKYfAnOlcJ"
      },
      "outputs": [],
      "source": [
        "import ply.lex as lex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx_dqWwkOlcL"
      },
      "source": [
        "A primeira coisa que o nosso analisador léxico (ou *lexer*/*tokenizer*) precisa de ter é uma lista de *tokens*. Como exemplo, vamos definir um *lexer* que lê expressões aritméticas, como \"4 * (2 + 3)\". Neste exemplo já somos capazes de identificar alguns *tokens*..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgXrbPicOlcM"
      },
      "outputs": [],
      "source": [
        "tokens = (\n",
        "    'NUMBER',\n",
        "    'PLUS',\n",
        "    # completar...\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsNXH-ZeOlcM"
      },
      "source": [
        "A seguir é preciso especificar cada *token*. Por outras palavras, precisamos de definir expressões regulares que permitam ao *tokenizer* identificar os *tokens*. Podemos fazê-lo através de variáveis ou de funções."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F95mz8RYOlcb"
      },
      "outputs": [],
      "source": [
        "t_PLUS = r'\\+'\n",
        "# completar...\n",
        "\n",
        "def t_NUMBER(t):\n",
        "    # completar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se alguma ação de processamento sobre o valor do token for necessária temos de o especiicar numa função.\n",
        "\n",
        "Por exemplo:\n",
        "\n",
        "```python\n",
        "def t_NUMBER(t):\n",
        "    r'\\d+'\n",
        "    t.value = int(t.value)\n",
        "    return t\n",
        "```\n",
        "\n",
        "Estas funções recebem um parâmetro que é uma instância de `LexToken`, que por sua vez é um objeto com os atributos:\n",
        "* **type** - Tipo do token, indicado pelo identificador que associamos ao token;\n",
        "* **value** - A string que fez match com a expressão regular;\n",
        "* **lineno** - O número da linha que está a ser processada;\n",
        "* **lexpos** - Posição relativa do token relativamente ao início do texto que está a ser processado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow2ILFtkOlcc"
      },
      "source": [
        "Podemos especificar um conjunto de caracteres que o analisador léxico vai ignorar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fxalLuTOlcd"
      },
      "outputs": [],
      "source": [
        "t_ignore = ' \\t\\n'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b9xt-tqOlce"
      },
      "source": [
        "Precisamos ainda de definir o comportamento do *tokenizer* caso encontre um carácter ou sequência de caracteres que não corresponda a nenhum *token* conhecido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYqjFDTEOlcp"
      },
      "outputs": [],
      "source": [
        "def t_error(t):\n",
        "    print(f\"Carácter ilegal {t.value[0]}\")\n",
        "    t.lexer.skip(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hMOGfxsOldI"
      },
      "source": [
        "Agora, já somos capazes de construir o nosso analisador léxico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN9vAe7UOldP"
      },
      "outputs": [],
      "source": [
        "lexer = lex.lex()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjACXglAOldP"
      },
      "source": [
        "Para o usar, precisamos de lhe dar algum valor de *input* e depois pedir-lhe para ir devolvendo os *tokens* que encontrar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5BEZj6XOldQ"
      },
      "outputs": [],
      "source": [
        "data = '''\n",
        "3 + 4 * 10\n",
        "  + -20 *2\n",
        "'''\n",
        "\n",
        "lexer.input(data)\n",
        "\n",
        "while tok := lexer.token():\n",
        "    print(tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Uvix55yOldz"
      },
      "source": [
        "Se quisermos manter informação sobre as linhas nas quais os *tokens* aparecem, podemos usar o atributo `lineno`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5CFURcLOleU"
      },
      "outputs": [],
      "source": [
        "t_ignore = ' \\t'\n",
        "\n",
        "def t_newline(t):\n",
        "    r'\\n+'\n",
        "    t.lexer.lineno += len(t.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Debug\n",
        "\n",
        "Podemos ativar esta opção para obtermos mais documentação sobre a geração do analisador léxico:\n",
        "\n",
        "```python\n",
        "lexer = lex.lex(debug=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modularização\n",
        "\n",
        "É possível isolar o analisador léxico num módulo:\n",
        "\n",
        "```python\n",
        "# módulo: o meu analisador léxico - analex.py\n",
        "\n",
        "tokens = (...)\n",
        "...\n",
        "def t_error(t):\n",
        "    ...\n",
        "```\n",
        "\n",
        "E depois usá-lo num programa da forma tradicional, ou na forma preparada no `Ply`:\n",
        "\n",
        "```python \n",
        "import analex\n",
        "\n",
        "lexer = lex.lex(module=analex)\n",
        "lexer.input(...)\n",
        "for tok in lexer:\n",
        "    print(tok)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPrQMVnjOldQ"
      },
      "source": [
        "É possível consultar a documentação do *lex.py* em https://ply.readthedocs.io/en/latest/ply.html#lex.\n",
        "\n",
        "Juntando tudo..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo das expressões aritméticas\n",
        "\n",
        "import ply.lex as lex\n",
        "\n",
        "tokens = (\n",
        "    'NUMBER',\n",
        "    'PLUS',\n",
        "    # completar...\n",
        ")\n",
        "\n",
        "t_PLUS = r'\\+'\n",
        "# completar...\n",
        "\n",
        "def t_NUMBER(t):\n",
        "    # completar\n",
        "    pass\n",
        "\n",
        "t_ignore = ' \\t\\n'\n",
        "\n",
        "def t_error(t):\n",
        "    print(f\"Carácter ilegal {t.value[0]}\")\n",
        "    t.lexer.skip(1)\n",
        "\n",
        "lexer = lex.lex()\n",
        "\n",
        "# -------------------------------------------\n",
        "\n",
        "data = '''\n",
        "3 + 4 * 10\n",
        "  + -20 *2\n",
        "'''\n",
        "\n",
        "lexer.input(data)\n",
        "\n",
        "for tok in lexer:\n",
        "    print(tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prioridades:\n",
        "\n",
        "Quando o `Ply`junta tudo para construir a expressão regular que irá fazer o reconhecimento de todos os tokens segue as seguintes regras:\n",
        "1. Os tokens definidos por funções são colocados em primeiro lugar pela ordem da definição das funções;\n",
        "2. Os tokens definidos por strings são colocados a seguir ordenados por ordem decrescente do tamanho da respetiva expressão regular."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyge6OSnOldR"
      },
      "source": [
        "## Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Aquecimento \n",
        "\n",
        "Especifica um analisador léxico para este excerto de uma linguagem de programação:\n",
        "\n",
        "```\n",
        "? a\n",
        "? b\n",
        "c = a*b/2\n",
        "d = f(c,2)\n",
        "! d\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "texto = \"\"\"\n",
        "? a\n",
        "? b\n",
        "c = a*b/2\n",
        "d = f(c,2)\n",
        "\"\"\"\n",
        "\n",
        "import ply.lex as lex\n",
        "\n",
        "tokens = (\n",
        "   'NUM',\n",
        "   'MUL',\n",
        "   'DIV',\n",
        "   'PA',\n",
        "   'PF',\n",
        "   'READ',\n",
        "   'ATRIB',\n",
        "   'VIRG',\n",
        "   'ID'\n",
        ")\n",
        "\n",
        "# Regular expression rules for simple tokens\n",
        "t_READ    = r'\\?'\n",
        "t_ATRIB   = r'='\n",
        "t_MUL     = r'\\*'\n",
        "t_DIV     = r'\\/'\n",
        "t_PA      = r'\\('\n",
        "t_PF      = r'\\)'\n",
        "t_VIRG    = r','\n",
        "\n",
        "# A regular expression rule with some action code\n",
        "def t_NUM(t):\n",
        "    r'\\d+'\n",
        "    t.value = int(t.value)\n",
        "    return t\n",
        "\n",
        "def t_ID(t):\n",
        "    r'[a-zA-Z_]\\w*'\n",
        "    return t\n",
        "\n",
        "# Define a rule so we can track line numbers\n",
        "def t_newline(t):\n",
        "    r'\\n+'\n",
        "    t.lexer.lineno += len(t.value)\n",
        "\n",
        "# A string containing ignored characters (spaces and tabs)\n",
        "t_ignore  = ' \\t'\n",
        "\n",
        "# Error handling rule\n",
        "def t_error(t):\n",
        "    print(f\"Illegal character {t.value[0]}\")\n",
        "    t.lexer.skip(1)\n",
        "\n",
        "# Build the lexer\n",
        "lexer = lex.lex()\n",
        "\n",
        "lexer.input(texto)\n",
        "for tok in lexer:\n",
        "    print(tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Desafio: colocar o exemplo anterior a processar o stdin\n",
        "\n",
        "* Isola o analisador léxico num módulo Python;\n",
        "* Cria um programa que importa o módulo e processa o stdin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Processando o stdin:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3hIQxn8OldR"
      },
      "source": [
        "### 1. Frases\n",
        "\n",
        "Define um analisador léxico capaz de ler uma frase e de identificar os seus componentes (palavras, vírgulas, sinais de pontuação)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UST_08V9OldS"
      },
      "source": [
        "### 2. Listas Mistas\n",
        "\n",
        "Define um analisador léxico capaz de receber listas com números, palavras ou valores booleanos como input (e.g.: `[ 1,5, palavra, False ,3.14,   0, fim]`) e identificar os seus *tokens*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s__Lct3Oldh"
      },
      "source": [
        "### 3. JSON\n",
        "\n",
        "Define um analisador léxico capaz de ler ficheiros em formato JSON e identificar os seus *tokens*.\n",
        "\n",
        "Exemplo de um documento JSON:\n",
        "\n",
        "---\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"John Doe\",\n",
        "  \"age\": 21,\n",
        "  \"gender\": \"male\",\n",
        "  \"height\": 1.68,\n",
        "  \"address\": {\n",
        "    \"street\": \"123 Main Street\",\n",
        "    \"city\": \"New York\",\n",
        "    \"country\": \"USA\",\n",
        "    \"zip\": \"10001\"\n",
        "  },\n",
        "  \"married\": false,\n",
        "  \"hobbies\": [\n",
        "    {\n",
        "      \"name\": \"reading\",\n",
        "      \"books\": [\n",
        "        {\n",
        "          \"title\": \"Heartstopper: Volume 1\",\n",
        "          \"author\": \"Alice Oseman\",\n",
        "          \"genres\": [\"Graphic Novels\", \"Romance\", \"Queer\"]\n",
        "        },\n",
        "        {\n",
        "          \"title\": \"1984\",\n",
        "          \"author\": \"George Orwell\",\n",
        "          \"genres\": [\"Science Fiction\", \"Dystopia\", \"Politics\"]\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"name\": \"gaming\",\n",
        "      \"games\": [\n",
        "        {\n",
        "          \"title\": \"Portal 2\",\n",
        "          \"platform\": [\"PC\", \"PlayStation 3\", \"Xbox 360\"]\n",
        "        },\n",
        "        {\n",
        "          \"title\": \"Synth Riders\",\n",
        "          \"platform\": [\"PSVR\", \"PSVR2\", \"PCVR\", \"Oculus Quest\"]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTLcLtKAOldi"
      },
      "source": [
        "## Condições de contexto\n",
        "\n",
        "Para certos analisadores léxico, pode ser útil ter diferentes estados. Por exemplo, se definirmos um analisador léxico para um ficheiro XML, pode ser útil verificar se o nome usado para fechar uma *tag* foi o mesmo que foi usado para a abrir.\n",
        "\n",
        "Exemplo de parte de um ficheiro XML:\n",
        "\n",
        "```xml\n",
        "<pessoa>\n",
        "    <nome>Maria</nome>\n",
        "    <idade>32</idade>\n",
        "</pessoa>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDH2v3NjOldw"
      },
      "outputs": [],
      "source": [
        "import ply.lex as lex\n",
        "\n",
        "states = (\n",
        "    ('taga', 'exclusive'),\n",
        "    ('tagf', 'exclusive'), # num estado exclusivo, apenas aplicamos os tokens e regras para esse estado\n",
        "                           # por outro lado, num estado inclusivo, as regras e tokens desse estado juntam-se às outras regras e tokens\n",
        "                           # o estado inicial chama-se 'INITIAL' e não é preciso defini-lo\n",
        ")\n",
        "\n",
        "tokens = (\n",
        "    'ABRIR_TAG',\n",
        "    'ABRIR_TAG_F',\n",
        "    'FECHAR_TAG',\n",
        "    'NOME_TAG',\n",
        "    'VALOR'\n",
        ")\n",
        "\n",
        "t_ignore = ' \\t\\n' # estes tokens apenas são ignorados no estado 'INITIAL' e em estados inclusivos\n",
        "\n",
        "t_VALOR = r'[^<]+'\n",
        "\n",
        "def t_ABRIR_TAG_F(t):\n",
        "    r'</'\n",
        "    t.lexer.begin('tagf') # entramos no estado 'tagf'\n",
        "    return t\n",
        "\n",
        "def t_ABRIR_TAG(t):\n",
        "    r'<'\n",
        "    t.lexer.begin('taga') # entramos no estado 'taga'\n",
        "    return t\n",
        "\n",
        "def t_taga_tagf_FECHAR_TAG(t):\n",
        "    r'>'\n",
        "    t.lexer.begin('INITIAL') # voltamos ao estado inicial\n",
        "    return t\n",
        "\n",
        "def t_taga_NOME_TAG(t):\n",
        "    r'\\w+'\n",
        "    t.lexer.stack.append(t.value)\n",
        "    return t\n",
        "\n",
        "def t_tagf_NOME_TAG(t):\n",
        "    r'\\w+'\n",
        "    if len(t.lexer.stack) > 0:\n",
        "        if (nt := t.lexer.stack.pop(-1)) != t.value:\n",
        "            print(f\"Erro - esperado nome de tag '{nt}', mas foi lido '{t.value}'!\")\n",
        "    else:\n",
        "        print(\"Erro - nenhuma tag aberta!\")\n",
        "    return t\n",
        "\n",
        "def t_ANY_error(t): # regra válida para todos os estados\n",
        "    print(f\"Carácter ilegal: {t.value[0]}\")\n",
        "    t.lexer.skip(1)\n",
        "\n",
        "\n",
        "data = '''\n",
        "<pessoa>\n",
        "    <nome>Maria</nome>\n",
        "    <idade>32</idade>\n",
        "</pessoa>\n",
        "'''\n",
        "\n",
        "lexer = lex.lex()\n",
        "\n",
        "lexer.stack = list() # vamos usar esta lista como stack para verificar os nomes das tags\n",
        "\n",
        "lexer.input(data)\n",
        "\n",
        "while tok := lexer.token():\n",
        "    print(tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJqjWeSfOleV"
      },
      "source": [
        "## Exercícios 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onMLXP8gOleY"
      },
      "source": [
        "### 1. BibTeX\n",
        "\n",
        "Define um analisador léxico capaz de ler um ficheiro no formato *BibTeX* e identificar os seus *tokens*.\n",
        "\n",
        "Exemplo de um ficheiro BibTeX:\n",
        "\n",
        "---\n",
        "\n",
        "```bibtex\n",
        "@incollection {HDYE78,\n",
        "author = \"Ricardo Martini and Pedro Rangel Henriques and Giovani Libreloto\",\n",
        "title = \"Storing Archival Emigration Documents to Create Virtual Exhibition Rooms\",\n",
        "booktitle = \"New Contributions in Information Systems and Technologies\",\n",
        "series=\"Advances in Intelligent Systems and Computing\",\n",
        "editor=\"Rocha, Alvaro and Correia, Ana and Costanzo, S. and Reis, Luis Paulo\",\n",
        "volume=\"353\",\n",
        "pages=\"403-409\",\n",
        "year = \"2015\",\n",
        "month =  \"April\"\n",
        "}\n",
        "\n",
        "\n",
        "@book {H787,\n",
        "author = {Vitor T. Martins and Pedro Rangel Henriques and Daniela da Cruz},\n",
        "title = {An AST-based tool, Spector, for Plagiarism Detection},\n",
        "booktitle = {Proceedings of SLATE’15},\n",
        "pages = {173--178},\n",
        "ISBN = {},\n",
        "year = {2015},\n",
        "month =   {},\n",
        "publisher = {Fundacion General UCM},\n",
        "annote = {Keywords: software, plagiarism, detection, comparison, test}}\n",
        "\n",
        "@book {H787,\n",
        "author = {Vitor T. Martins and Pedro Rangel Henriques and Daniela da Cruz},\n",
        "title = \"{A}n {AST}-based tool, {S}pector, for Plagiarism Detection\",\n",
        "booktitle = {Proceedings of SLATE’15},\n",
        "pages = {173--178},\n",
        "ISBN = {},\n",
        "year = {2015},\n",
        "month =   {},\n",
        "publisher = {Fundaci ́on General UCM},\n",
        "annote = {Keywords: software, plagiarism, detection, comparison, test}\n",
        "}\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxXn9uTSOlep"
      },
      "source": [
        "### 2. Somador on/off\n",
        "\n",
        "Usando um analisador léxico com condições de contexto, cria um programa em Python que tenha o seguinte comportamento:\n",
        "\n",
        "* Pretende-se um programa que some todas as sequências de dígitos que encontre num texto;\n",
        "* Prepara o programa para ler o texto do canal de entrada: stdin;\n",
        "* Sempre que encontrar a string “Off” em qualquer combinação de maiúsculas e minúsculas, esse comportamento é desligado;\n",
        "* Sempre que encontrar a string “On” em qualquer combinação de maiúsculas e minúsculas, esse comportamento é novamente ligado;\n",
        "* Sempre que encontrar o caráter “=”, o resultado da soma é colocado na saída."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQePKCJEg8Oj"
      },
      "source": [
        "### 3. Removedor de Comentários\n",
        "\n",
        "Desenvolve um analisador léxico capaz de ler um ficheiro de texto e ignorar todo o texto dentro de comentários inline (desde \"//\" até ao fim de linha) e todo o texto dentro de comentários multiline (desde \"/*\" até \"*/\").\n",
        "\n",
        "O *lexer* deve suportar convenientemente comentários dentro de comentários,   conforme exemplificado abaixo:\n",
        "\n",
        "---\n",
        "```c\n",
        "/* comment */ ola1\n",
        "\n",
        "/* comment****comment */ ola2 /*\n",
        "comment\n",
        "/* comentário dentro de comentário */\n",
        "****/ ola3\n",
        "\n",
        "/*********/\n",
        "\n",
        "ola4\n",
        " mais um pouco // remover comentário inline\n",
        "FIM\n",
        "```\n",
        "----\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5994ec7557f9f4a3c4b5d4f7132f657ee5f793fd2b9e6534077474582be4b489"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
