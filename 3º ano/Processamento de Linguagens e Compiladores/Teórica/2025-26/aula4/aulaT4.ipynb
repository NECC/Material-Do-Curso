{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula Teórica 4 (guião)\n",
    "### Semana de 6 a 10 de Outubro 2025\n",
    "### José Carlos Ramalho\n",
    "### Sinopsis:\n",
    "\n",
    "* Conversão de AFND em AFD;\n",
    "* Análise Léxica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de Léxica \n",
    "\n",
    "Resumindo, é um bloco que recebe uma lista de carateres (`[Char]`) e produz uma lista de tokens (`[Token]`).\n",
    "\n",
    "Vamos exemplificar com a linguagem dos parentesis.\n",
    "\n",
    "Exemplos de frases corretas:\n",
    "\n",
    "``` \n",
    "()\n",
    "(())()\n",
    "((()()())())(())\n",
    "```\n",
    "\n",
    "Exemplos de frases incorretas:\n",
    "\n",
    "```\n",
    ")\n",
    "(()))\n",
    "())()\n",
    "(())()(()\n",
    "```\n",
    "\n",
    "Independentemente da correção da frase, a partir da entrada queremos obter uma lista diferente:\n",
    "\n",
    "```\n",
    "In: (())()\n",
    "Out: PA PA PF PF PA PF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementação: como fazer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consumir o input\n",
    "import sys\n",
    "\n",
    "for linha in sys.stdin:\n",
    "    for tok in tokenize(linha):\n",
    "        print(tok)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a função tokenize\n",
    "import re\n",
    "\n",
    "# Tokens a reconhecer: PA, PF, SKIP, NEWLINE, ERRO\n",
    "\n",
    "def tokenize(input_string):\n",
    "    reconhecidos = []\n",
    "    linha = 1\n",
    "    mo = re.finditer(r'(?P<PA>\\()|(?P<PF>\\))|(?P<SKIP>[ \\t])|(?P<NEWLINE>\\n)|(?P<ERRO>.)', input_string)\n",
    "    for m in mo:\n",
    "        dic = m.groupdict()\n",
    "        if dic['PA']:\n",
    "            t = (\"PA\", dic['PA'], linha, m.span())\n",
    "\n",
    "        elif dic['PF']:\n",
    "            t = (\"PF\", dic['PF'], linha, m.span())\n",
    "    \n",
    "        elif dic['SKIP']:\n",
    "            t = (\"SKIP\", dic['SKIP'], linha, m.span())\n",
    "    \n",
    "        elif dic['NEWLINE']:\n",
    "            t = (\"NEWLINE\", dic['NEWLINE'], linha, m.span())\n",
    "    \n",
    "        elif dic['ERRO']:\n",
    "            t = (\"ERRO\", dic['ERRO'], linha, m.span())\n",
    "    \n",
    "        else:\n",
    "            t = (\"UNKNOWN\", m.group(), linha, m.span())\n",
    "        if not dic['SKIP'] and t[0] != 'UNKNOWN': reconhecidos.append(t)\n",
    "    return reconhecidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste:\n",
    "\n",
    "```\n",
    "$ python3 analex_par.py\n",
    "(())\n",
    "('PA', '(', 1, (0, 1))\n",
    "('PA', '(', 1, (1, 2))\n",
    "('PF', ')', 1, (2, 3))\n",
    "('PF', ')', 1, (3, 4))\n",
    "('NEWLINE', '\\n', 1, (4, 5))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mais um exemplo: reconhecer sequências de dígitos no texto de entrada \n",
    "\n",
    "```\n",
    "In: Olha, vai às compras e compra: 2 peras, 4 bananas e 5 tangerinas.\n",
    "Out: INT INT INT\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "\n",
    "def tokenize(input_string):\n",
    "    reconhecidos = []\n",
    "    linha = 1\n",
    "    mo = re.finditer(r'(?P<INT>\\d+)|(?P<SKIP>[ \\t]|.)|(?P<NEWLINE>\\n)|(?P<ERRO>.)', input_string)\n",
    "    for m in mo:\n",
    "        dic = m.groupdict()\n",
    "        if dic['INT']:\n",
    "            t = (\"INT\", dic['INT'], linha, m.span())\n",
    "    \n",
    "        elif dic['SKIP']:\n",
    "            t = (\"SKIP\", dic['SKIP'], linha, m.span())\n",
    "    \n",
    "        elif dic['NEWLINE']:\n",
    "            t = (\"NEWLINE\", dic['NEWLINE'], linha, m.span())\n",
    "    \n",
    "        elif dic['ERRO']:\n",
    "            t = (\"ERRO\", dic['ERRO'], linha, m.span())\n",
    "    \n",
    "        else:\n",
    "            t = (\"UNKNOWN\", m.group(), linha, m.span())\n",
    "        if not dic['SKIP'] and t[0] != 'UNKNOWN': reconhecidos.append(t)\n",
    "    return reconhecidos\n",
    "\n",
    "for linha in sys.stdin:\n",
    "    for tok in tokenize(linha):\n",
    "        print(tok)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python3 analex_int.py\n",
    "Olha, vai às compras e compra: 2 peras, 4 bananas e 5 tangerinas.\n",
    "('INT', '2', 1, (31, 32))\n",
    "('INT', '4', 1, (40, 41))\n",
    "('INT', '5', 1, (52, 53))\n",
    "('NEWLINE', '\\n', 1, (65, 66))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalização\n",
    "\n",
    "O processo é sempre o mesmo pelo que pode ser generalizado bastando isolar a definição dos tokens num ficheiro à parte.\n",
    "\n",
    "Optou-se por usar um ficheiro JSON para isso (devido ao formato teremos de usar strings normais).\n",
    "\n",
    "A seguir demonstra-se o processo com o exemplo das expressões aritméticas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição dos tokens\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"id\": \"ADD\",\n",
    "        \"expreg\": \"\\\\+\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"SUB\",\n",
    "        \"expreg\": \"\\\\-\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"MUL\",\n",
    "        \"expreg\": \"\\\\*\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"DIV\",\n",
    "        \"expreg\": \"\\\\/*\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"INT\",\n",
    "        \"expreg\": \"\\\\d+\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"SKIP\",\n",
    "        \"expreg\": \"[ \\\\t]\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"NEWLINE\",\n",
    "        \"expreg\": \"\\\\n\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"ERRO\",\n",
    "        \"expreg\": \".\"\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O Gerador de Analisadores Léxicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "\n",
    "def main():\n",
    "    # Ensure a filename was provided as a command-line argument\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python gen_tokenizer2.py <filename>\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    filename = sys.argv[1]\n",
    "\n",
    "    # Try to open and load the specified file\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            tokens = json.load(f)\n",
    "           \n",
    "        tokens_regex = '|'.join([f'(?P<{t['id']}>{t['expreg']})' for t in tokens])\n",
    "\n",
    "        code = f\"\"\"\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def tokenize(input_string):\n",
    "    reconhecidos = []\n",
    "    linha = 1\n",
    "    mo = re.finditer(r'{tokens_regex}', input_string)\n",
    "    for m in mo:\n",
    "        dic = m.groupdict()\n",
    "        if dic['{tokens[0]['id']}']:\n",
    "            t = (\"{tokens[0]['id']}\", dic['{tokens[0]['id']}'], linha, m.span())\n",
    "\"\"\"\n",
    "\n",
    "        for t in tokens[1:]:\n",
    "            code += f\"\"\"\n",
    "        elif dic['{t['id']}']:\n",
    "            t = (\"{t['id']}\", dic['{t['id']}'], linha, m.span())\n",
    "    \"\"\"\n",
    "        code += f\"\"\"\n",
    "        else:\n",
    "            t = (\"UNKNOWN\", m.group(), linha, m.span())\n",
    "        if not dic['SKIP'] and t[0] != 'UNKNOWN': reconhecidos.append(t)\n",
    "    return reconhecidos\n",
    "\n",
    "for linha in sys.stdin:\n",
    "    for tok in tokenize(linha):\n",
    "        print(tok)    \n",
    "\"\"\"\n",
    "        print(code)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found.\")\n",
    "        sys.exit(1)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: File '{filename}' is not valid JSON.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilização \n",
    "\n",
    "Passos para a utilização do gerador:\n",
    "\n",
    "1. `$ python3 gen_tokenizer2.py tokens_exp.json > analex_exp.py`\n",
    "2. `$ python3 analex_exp.py`\n",
    "3. \n",
    "\n",
    "```\n",
    "$ python3 gen_tokenizer2.py tokens_exp.json > analex_exp.py\n",
    "$ python3 analex_exp.py \n",
    "45+7*8\n",
    "('INT', '45', 1, (0, 2))\n",
    "('ADD', '+', 1, (2, 3))\n",
    "('INT', '7', 1, (3, 4))\n",
    "('MUL', '*', 1, (4, 5))\n",
    "('INT', '8', 1, (5, 6))\n",
    "('NEWLINE', '\\n', 1, (6, 7))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
